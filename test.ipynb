{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thanh/miniconda3/envs/sckd/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "from sampler import data_sampler\n",
    "from config import Config\n",
    "import torch\n",
    "from model.bert_encoder import Bert_Encoder\n",
    "from model.dropout_layer import Dropout_Layer\n",
    "from model.classifier import Softmax_Layer, Proto_Softmax_Layer\n",
    "from data_loader import get_data_loader\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import collections\n",
    "from copy import deepcopy\n",
    "import os\n",
    "# import wandb\n",
    "# os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "\n",
    "\n",
    "def train_simple_model(config, encoder, dropout_layer, classifier, training_data, epochs, map_relid2tempid):\n",
    "    data_loader = get_data_loader(config, training_data, shuffle=True)\n",
    "\n",
    "    encoder.train()\n",
    "    dropout_layer.train()\n",
    "    classifier.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': encoder.parameters(), 'lr': 0.00001},\n",
    "        {'params': dropout_layer.parameters(), 'lr': 0.00001},\n",
    "        {'params': classifier.parameters(), 'lr': 0.001}\n",
    "    ])\n",
    "    for epoch_i in range(epochs):\n",
    "        losses = []\n",
    "        for step, batch_data in enumerate(data_loader):\n",
    "            optimizer.zero_grad()\n",
    "            labels, _, tokens = batch_data\n",
    "            labels = labels.to(config.device)\n",
    "            labels = [map_relid2tempid[x.item()] for x in labels]\n",
    "            labels = torch.tensor(labels).to(config.device)\n",
    "\n",
    "            tokens = torch.stack([x.to(config.device) for x in tokens],dim=0)\n",
    "            reps = encoder(tokens)\n",
    "            reps, _ = dropout_layer(reps)\n",
    "            logits = classifier(reps)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"loss is {np.array(losses).mean()}\")\n",
    "\n",
    "\n",
    "def compute_jsd_loss(m_input):\n",
    "    # m_input: the result of m times dropout after the classifier.\n",
    "    # size: m*B*C\n",
    "    m = m_input.shape[0]\n",
    "    mean = torch.mean(m_input, dim=0)\n",
    "    jsd = 0\n",
    "    for i in range(m):\n",
    "        loss = F.kl_div(F.log_softmax(mean, dim=-1), F.softmax(m_input[i], dim=-1), reduction='none')\n",
    "        loss = loss.sum()\n",
    "        jsd += loss / m\n",
    "    return jsd\n",
    "\n",
    "\n",
    "def contrastive_loss(hidden, labels):\n",
    "\n",
    "    logsoftmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    return -(logsoftmax(hidden) * labels).sum() / labels.sum()\n",
    "\n",
    "\n",
    "def construct_hard_triplets(output, labels, relation_data):\n",
    "    positive = []\n",
    "    negative = []\n",
    "    pdist = nn.PairwiseDistance(p=2)\n",
    "    for rep, label in zip(output, labels):\n",
    "        positive_relation_data = relation_data[label.item()]\n",
    "        negative_relation_data = []\n",
    "        for key in relation_data.keys():\n",
    "            if key != label.item():\n",
    "                negative_relation_data.extend(relation_data[key])\n",
    "        positive_distance = torch.stack([pdist(rep.cpu(), p) for p in positive_relation_data])\n",
    "        negative_distance = torch.stack([pdist(rep.cpu(), n) for n in negative_relation_data])\n",
    "        positive_index = torch.argmax(positive_distance)\n",
    "        negative_index = torch.argmin(negative_distance)\n",
    "        positive.append(positive_relation_data[positive_index.item()])\n",
    "        negative.append(negative_relation_data[negative_index.item()])\n",
    "\n",
    "\n",
    "    return positive, negative\n",
    "\n",
    "\n",
    "def train_first(config, encoder, dropout_layer, classifier, training_data, epochs, map_relid2tempid, new_relation_data):\n",
    "    data_loader = get_data_loader(config, training_data, shuffle=True)\n",
    "\n",
    "    encoder.train()\n",
    "    dropout_layer.train()\n",
    "    classifier.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': encoder.parameters(), 'lr': 0.00001},\n",
    "        {'params': dropout_layer.parameters(), 'lr': 0.00001},\n",
    "        {'params': classifier.parameters(), 'lr': 0.001}\n",
    "    ])\n",
    "    triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "    for epoch_i in range(epochs):\n",
    "        losses = []\n",
    "        for step, (labels, _, tokens) in enumerate(data_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_all = []\n",
    "            tokens = torch.stack([x.to(config.device) for x in tokens], dim=0)\n",
    "            labels = labels.to(config.device)\n",
    "            origin_labels = labels[:]\n",
    "            labels = [map_relid2tempid[x.item()] for x in labels]\n",
    "            labels = torch.tensor(labels).to(config.device)\n",
    "            reps = encoder(tokens)\n",
    "            outputs,_ = dropout_layer(reps)\n",
    "            positives,negatives = construct_hard_triplets(outputs, origin_labels, new_relation_data)\n",
    "\n",
    "            for _ in range(config.f_pass):\n",
    "                output, output_embedding = dropout_layer(reps)\n",
    "                logits = classifier(output)\n",
    "                logits_all.append(logits)\n",
    "\n",
    "            positives = torch.cat(positives, 0).to(config.device)\n",
    "            negatives = torch.cat(negatives, 0).to(config.device)\n",
    "            anchors = outputs\n",
    "            logits_all = torch.stack(logits_all)\n",
    "            m_labels = labels.expand((config.f_pass, labels.shape[0]))  # m,B\n",
    "            loss1 = criterion(logits_all.reshape(-1, logits_all.shape[-1]), m_labels.reshape(-1))\n",
    "            loss2 = compute_jsd_loss(logits_all)\n",
    "            tri_loss = triplet_loss(anchors, positives, negatives)\n",
    "            loss = loss1 + loss2 + tri_loss\n",
    "\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        print(f\"loss is {np.array(losses).mean()}\")\n",
    "\n",
    "\n",
    "def train_mem_model(config, encoder, dropout_layer, classifier, training_data, epochs, map_relid2tempid, new_relation_data,\n",
    "                prev_encoder, prev_dropout_layer, prev_classifier, prev_relation_index):\n",
    "    data_loader = get_data_loader(config, training_data, shuffle=True)\n",
    "\n",
    "    encoder.train()\n",
    "    dropout_layer.train()\n",
    "    classifier.train()\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam([\n",
    "        {'params': encoder.parameters(), 'lr': 0.00001},\n",
    "        {'params': dropout_layer.parameters(), 'lr': 0.00001},\n",
    "        {'params': classifier.parameters(), 'lr': 0.001}\n",
    "    ])\n",
    "    triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)\n",
    "    distill_criterion = nn.CosineEmbeddingLoss()\n",
    "    T = config.kl_temp\n",
    "    for epoch_i in range(epochs):\n",
    "        losses = []\n",
    "        for step, (labels, _, tokens) in enumerate(data_loader):\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits_all = []\n",
    "            tokens = torch.stack([x.to(config.device) for x in tokens], dim=0)\n",
    "            labels = labels.to(config.device)\n",
    "            origin_labels = labels[:]\n",
    "            labels = [map_relid2tempid[x.item()] for x in labels]\n",
    "            labels = torch.tensor(labels).to(config.device)\n",
    "            reps = encoder(tokens)\n",
    "            normalized_reps_emb = F.normalize(reps.view(-1, reps.size()[1]), p=2, dim=1)\n",
    "            outputs,_ = dropout_layer(reps)\n",
    "            if prev_dropout_layer is not None:\n",
    "                prev_outputs, _ = prev_dropout_layer(reps)\n",
    "                positives,negatives = construct_hard_triplets(prev_outputs, origin_labels, new_relation_data)\n",
    "            else:\n",
    "                positives, negatives = construct_hard_triplets(outputs, origin_labels, new_relation_data)\n",
    "\n",
    "            for _ in range(config.f_pass):\n",
    "                output, output_embedding = dropout_layer(reps)\n",
    "                logits = classifier(output)\n",
    "                logits_all.append(logits)\n",
    "\n",
    "            positives = torch.cat(positives, 0).to(config.device)\n",
    "            negatives = torch.cat(negatives, 0).to(config.device)\n",
    "            anchors = outputs\n",
    "            logits_all = torch.stack(logits_all)\n",
    "            m_labels = labels.expand((config.f_pass, labels.shape[0]))  # m,B\n",
    "            loss1 = criterion(logits_all.reshape(-1, logits_all.shape[-1]), m_labels.reshape(-1))\n",
    "            loss2 = compute_jsd_loss(logits_all)\n",
    "            tri_loss = triplet_loss(anchors, positives, negatives)\n",
    "            loss = loss1 + loss2 + tri_loss\n",
    "\n",
    "            if prev_encoder is not None:\n",
    "                prev_reps = prev_encoder(tokens).detach()\n",
    "                normalized_prev_reps_emb = F.normalize(prev_reps.view(-1, prev_reps.size()[1]), p=2, dim=1)\n",
    "\n",
    "                feature_distill_loss = distill_criterion(normalized_reps_emb, normalized_prev_reps_emb,\n",
    "                                                         torch.ones(tokens.size(0)).to(\n",
    "                                                             config.device))\n",
    "                loss += feature_distill_loss\n",
    "\n",
    "            if prev_dropout_layer is not None and prev_classifier is not None:\n",
    "                prediction_distill_loss = None\n",
    "                dropout_output_all = []\n",
    "                prev_dropout_output_all = []\n",
    "                for i in range(config.f_pass):\n",
    "                    output, _ = dropout_layer(reps)\n",
    "                    prev_output, _ = prev_dropout_layer(reps)\n",
    "                    dropout_output_all.append(output)\n",
    "                    prev_dropout_output_all.append(output)\n",
    "                    pre_logits = prev_classifier(output).detach()\n",
    "\n",
    "                    pre_logits = F.softmax(pre_logits.index_select(1, prev_relation_index) / T, dim=1)\n",
    "\n",
    "                    log_logits = F.log_softmax(logits_all[i].index_select(1, prev_relation_index) / T, dim=1)\n",
    "                    if i == 0:\n",
    "                        prediction_distill_loss = -torch.mean(torch.sum(pre_logits * log_logits, dim=1))\n",
    "                    else:\n",
    "                        prediction_distill_loss += -torch.mean(torch.sum(pre_logits * log_logits, dim=1))\n",
    "\n",
    "                prediction_distill_loss /= config.f_pass\n",
    "                loss += prediction_distill_loss\n",
    "                dropout_output_all = torch.stack(dropout_output_all)\n",
    "                prev_dropout_output_all = torch.stack(prev_dropout_output_all)\n",
    "                mean_dropout_output_all = torch.mean(dropout_output_all, dim=0)\n",
    "                mean_prev_dropout_output_all = torch.mean(prev_dropout_output_all,dim=0)\n",
    "                normalized_output = F.normalize(mean_dropout_output_all.view(-1, mean_dropout_output_all.size()[1]), p=2, dim=1)\n",
    "                normalized_prev_output = F.normalize(mean_prev_dropout_output_all.view(-1, mean_prev_dropout_output_all.size()[1]), p=2, dim=1)\n",
    "                hidden_distill_loss = distill_criterion(normalized_output, normalized_prev_output,\n",
    "                                                         torch.ones(tokens.size(0)).to(\n",
    "                                                             config.device))\n",
    "                loss += hidden_distill_loss\n",
    "\n",
    "            loss.backward()\n",
    "            losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "        print(f\"loss is {np.array(losses).mean()}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def batch2device(batch_tuple, device):\n",
    "    ans = []\n",
    "    for var in batch_tuple:\n",
    "        if isinstance(var, torch.Tensor):\n",
    "            ans.append(var.to(device))\n",
    "        elif isinstance(var, list):\n",
    "            ans.append(batch2device(var))\n",
    "        elif isinstance(var, tuple):\n",
    "            ans.append(tuple(batch2device(var)))\n",
    "        else:\n",
    "            ans.append(var)\n",
    "    return ans\n",
    "\n",
    "\n",
    "def evaluate_strict_model(config, encoder, dropout_layer, classifier, test_data, seen_relations, map_relid2tempid):\n",
    "    data_loader = get_data_loader(config, test_data, batch_size=1)\n",
    "    encoder.eval()\n",
    "    dropout_layer.eval()\n",
    "    classifier.eval()\n",
    "    n = len(test_data)\n",
    "\n",
    "    correct = 0\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        labels, _, tokens = batch_data\n",
    "        labels = labels.to(config.device)\n",
    "        labels = [map_relid2tempid[x.item()] for x in labels]\n",
    "        labels = torch.tensor(labels).to(config.device)\n",
    "\n",
    "        tokens = torch.stack([x.to(config.device) for x in tokens],dim=0)\n",
    "        reps = encoder(tokens)\n",
    "        reps, _ = dropout_layer(reps)\n",
    "        logits = classifier(reps)\n",
    "\n",
    "        seen_relation_ids = [rel2id[relation] for relation in seen_relations]\n",
    "        seen_relation_ids = [map_relid2tempid[relation] for relation in seen_relation_ids]\n",
    "        seen_sim = logits[:,seen_relation_ids].cpu().data.numpy()\n",
    "        max_smi = np.max(seen_sim,axis=1)\n",
    "\n",
    "        label_smi = logits[:,labels].cpu().data.numpy()\n",
    "\n",
    "        if label_smi >= max_smi:\n",
    "            correct += 1\n",
    "\n",
    "    return correct/n\n",
    "\n",
    "\n",
    "def select_data(config, encoder, dropout_layer, relation_dataset):\n",
    "    data_loader = get_data_loader(config, relation_dataset, shuffle=False, drop_last=False, batch_size=1)\n",
    "    features = []\n",
    "    encoder.eval()\n",
    "    dropout_layer.eval()\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        labels, _, tokens = batch_data\n",
    "        tokens = torch.stack([x.to(config.device) for x in tokens],dim=0)\n",
    "        with torch.no_grad():\n",
    "            feature = dropout_layer(encoder(tokens))[1].cpu()\n",
    "        features.append(feature)\n",
    "\n",
    "    features = np.concatenate(features)\n",
    "    num_clusters = min(config.num_protos, len(relation_dataset))\n",
    "    distances = KMeans(n_clusters=num_clusters, random_state=0).fit_transform(features)\n",
    "\n",
    "    memory = []\n",
    "    for k in range(num_clusters):\n",
    "        sel_index = np.argmin(distances[:, k])\n",
    "        instance = relation_dataset[sel_index]\n",
    "        memory.append(instance)\n",
    "    return memory\n",
    "\n",
    "\n",
    "def get_proto(config, encoder, dropout_layer, relation_dataset):\n",
    "    data_loader = get_data_loader(config, relation_dataset, shuffle=False, drop_last=False, batch_size=1)\n",
    "    features = []\n",
    "    encoder.eval()\n",
    "    dropout_layer.eval()\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        labels, _, tokens = batch_data\n",
    "        tokens = torch.stack([x.to(config.device) for x in tokens],dim=0)\n",
    "        with torch.no_grad():\n",
    "            feature = dropout_layer(encoder(tokens))[1]\n",
    "        features.append(feature)\n",
    "    features = torch.cat(features, dim=0)\n",
    "    proto = torch.mean(features, dim=0, keepdim=True).cpu()\n",
    "    standard = torch.sqrt(torch.var(features, dim=0)).cpu()\n",
    "    return proto, standard\n",
    "\n",
    "\n",
    "def generate_relation_data(protos, relation_standard):\n",
    "    relation_data = {}\n",
    "    relation_sample_nums = 10\n",
    "    for id in protos.keys():\n",
    "        relation_data[id] = []\n",
    "        difference = np.random.normal(loc=0, scale=1, size=relation_sample_nums)\n",
    "        for diff in difference:\n",
    "            relation_data[id].append(protos[id] + diff * relation_standard[id])\n",
    "    return relation_data\n",
    "\n",
    "\n",
    "def generate_current_relation_data(config, encoder, dropout_layer, relation_dataset):\n",
    "    data_loader = get_data_loader(config, relation_dataset, shuffle=False, drop_last=False, batch_size=1)\n",
    "    relation_data = []\n",
    "    encoder.eval()\n",
    "    dropout_layer.eval()\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        labels, _, tokens = batch_data\n",
    "        tokens = torch.stack([x.to(config.device) for x in tokens],dim=0)\n",
    "        with torch.no_grad():\n",
    "            feature = dropout_layer(encoder(tokens))[1].cpu()\n",
    "        relation_data.append(feature)\n",
    "    return relation_data\n",
    "\n",
    "from transformers import  BertTokenizer\n",
    "def data_augmentation(config, encoder, train_data, prev_train_data):\n",
    "    expanded_train_data = train_data[:]\n",
    "    expanded_prev_train_data = prev_train_data[:]\n",
    "    encoder.eval()\n",
    "    all_data = train_data + prev_train_data\n",
    "    tokenizer = BertTokenizer.from_pretrained(config.bert_path, additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"])\n",
    "    entity_index = []\n",
    "    entity_mention = []\n",
    "    for sample in all_data:\n",
    "        e11 = sample['tokens'].index(30522)\n",
    "        e12 = sample['tokens'].index(30523)\n",
    "        e21 = sample['tokens'].index(30524)\n",
    "        e22 = sample['tokens'].index(30525)\n",
    "        entity_index.append([e11,e12])\n",
    "        entity_mention.append(sample['tokens'][e11+1:e12])\n",
    "        entity_index.append([e21,e22])\n",
    "        entity_mention.append(sample['tokens'][e21+1:e22])\n",
    "\n",
    "    data_loader = get_data_loader(config, all_data, shuffle=False, drop_last=False, batch_size=1)\n",
    "    features = []\n",
    "    encoder.eval()\n",
    "    for step, batch_data in enumerate(data_loader):\n",
    "        labels, _, tokens = batch_data\n",
    "        tokens = torch.stack([x.to(config.device) for x in tokens],dim=0)\n",
    "        with torch.no_grad():\n",
    "            feature = encoder(tokens)\n",
    "        feature1, feature2 = torch.split(feature, [config.encoder_output_size,config.encoder_output_size], dim=1)\n",
    "        features.append(feature1)\n",
    "        features.append(feature2)\n",
    "    features = torch.cat(features, dim=0)\n",
    "    # similarity_matrix = F.cosine_similarity(features.unsqueeze(1), features.unsqueeze(0), dim=-1)\n",
    "    similarity_matrix = []\n",
    "    for i in range(len(features)):\n",
    "        similarity_matrix.append([0]*len(features))\n",
    "\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i,len(features)):\n",
    "            similarity = F.cosine_similarity(features[i],features[j],dim=0)\n",
    "            similarity_matrix[i][j] = similarity\n",
    "            similarity_matrix[j][i] = similarity\n",
    "\n",
    "    similarity_matrix = torch.tensor(similarity_matrix).to(config.device)\n",
    "    zero = torch.zeros_like(similarity_matrix).to(config.device)\n",
    "    diag = torch.diag_embed(torch.diag(similarity_matrix))\n",
    "    similarity_matrix -= diag\n",
    "    similarity_matrix = torch.where(similarity_matrix<0.95, zero, similarity_matrix)\n",
    "    nonzero_index = torch.nonzero(similarity_matrix)\n",
    "    expanded_train_count = 0\n",
    "\n",
    "    for origin, replace in nonzero_index:\n",
    "        sample_index = int(origin/2)\n",
    "        sample = all_data[sample_index]\n",
    "        if entity_mention[origin] == entity_mention[replace]:\n",
    "            continue\n",
    "        new_tokens = sample['tokens'][:entity_index[origin][0]+1] + entity_mention[replace] + sample['tokens'][entity_index[origin][1]:]\n",
    "        if len(new_tokens) < config.max_length:\n",
    "            new_tokens = new_tokens + [0]*(config.max_length-len(new_tokens))\n",
    "        else:\n",
    "            new_tokens = new_tokens[:config.max_length]\n",
    "\n",
    "        new_sample = {\n",
    "            'relation': sample['relation'],\n",
    "            'neg_labels': sample['neg_labels'],\n",
    "            'tokens': new_tokens\n",
    "        }\n",
    "        if sample_index < len(train_data) and expanded_train_count < 5 * len(train_data):\n",
    "            expanded_train_data.append(new_sample)\n",
    "            expanded_train_count += 1\n",
    "        else:\n",
    "            expanded_prev_train_data.append(new_sample)\n",
    "    return expanded_train_data, expanded_prev_train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    task = \"fewrel\"\n",
    "    shot = 5\n",
    "    config = 'config.ini'\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config(args.config)\n",
    "config.device = torch.device(config.device)\n",
    "config.n_gpu = torch.cuda.device_count()\n",
    "config.batch_size_per_step = int(config.batch_size / config.gradient_accumulation_steps)\n",
    "\n",
    "config.task = args.task\n",
    "config.shot = args.shot\n",
    "config.step1_epochs = 5\n",
    "config.step2_epochs = 15\n",
    "config.step3_epochs = 20\n",
    "config.temperature = 0.08\n",
    "\n",
    "if config.task == \"FewRel\":\n",
    "    config.relation_file = \"data/fewrel/relation_name.txt\"\n",
    "    config.rel_index = \"data/fewrel/rel_index.npy\"\n",
    "    config.rel_feature = \"data/fewrel/rel_feature.npy\"\n",
    "    config.rel_des_file = \"data/fewrel/relation_description.txt\"\n",
    "    config.num_of_relation = 80\n",
    "    if config.shot == 5:\n",
    "        config.rel_cluster_label = \"data/fewrel/CFRLdata_10_100_10_5/rel_cluster_label_0.npy\"\n",
    "        config.training_file = \"data/fewrel/CFRLdata_10_100_10_5/train_0.txt\"\n",
    "        config.valid_file = \"data/fewrel/CFRLdata_10_100_10_5/valid_0.txt\"\n",
    "        config.test_file = \"data/fewrel/CFRLdata_10_100_10_5/test_0.txt\"\n",
    "    elif config.shot == 10:\n",
    "        config.rel_cluster_label = \"data/fewrel/CFRLdata_10_100_10_10/rel_cluster_label_0.npy\"\n",
    "        config.training_file = \"data/fewrel/CFRLdata_10_100_10_10/train_0.txt\"\n",
    "        config.valid_file = \"data/fewrel/CFRLdata_10_100_10_10/valid_0.txt\"\n",
    "        config.test_file = \"data/fewrel/CFRLdata_10_100_10_10/test_0.txt\"\n",
    "    else:\n",
    "        config.rel_cluster_label = \"data/fewrel/CFRLdata_10_100_10_2/rel_cluster_label_0.npy\"\n",
    "        config.training_file = \"data/fewrel/CFRLdata_10_100_10_2/train_0.txt\"\n",
    "        config.valid_file = \"data/fewrel/CFRLdata_10_100_10_2/valid_0.txt\"\n",
    "        config.test_file = \"data/fewrel/CFRLdata_10_100_10_2/test_0.txt\"\n",
    "else:\n",
    "    config.relation_file = \"data/tacred/relation_name.txt\"\n",
    "    config.rel_index = \"data/tacred/rel_index.npy\"\n",
    "    config.rel_feature = \"data/tacred/rel_feature.npy\"\n",
    "    config.num_of_relation = 41\n",
    "    if config.shot == 5:\n",
    "        config.rel_cluster_label = \"data/tacred/CFRLdata_10_100_10_5/rel_cluster_label_0.npy\"\n",
    "        config.training_file = \"data/tacred/CFRLdata_10_100_10_5/train_0.txt\"\n",
    "        config.valid_file = \"data/tacred/CFRLdata_10_100_10_5/valid_0.txt\"\n",
    "        config.test_file = \"data/tacred/CFRLdata_10_100_10_5/test_0.txt\"\n",
    "    else:\n",
    "        config.rel_cluster_label = \"data/tacred/CFRLdata_10_100_10_10/rel_cluster_label_0.npy\"\n",
    "        config.training_file = \"data/tacred/CFRLdata_10_100_10_10/train_0.txt\"\n",
    "        config.valid_file = \"data/tacred/CFRLdata_10_100_10_10/valid_0.txt\"\n",
    "        config.test_file = \"data/tacred/CFRLdata_10_100_10_10/test_0.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7 6 5 2 1 0 3 4]\n"
     ]
    }
   ],
   "source": [
    "sampler = data_sampler(config=config, seed=config.seed+100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['person countries of residence', 'organization top members employees', 'organization member of', 'person origin', 'person title', 'organization country of headquarters']\n",
      "['person stateorprovinces of residence', 'person date of death', 'organization number of employees members', 'person alternate names', 'person spouse']\n",
      "['person date of birth', 'person stateorprovince of birth', 'person parents', 'person employee of', 'person stateorprovince of death']\n",
      "['person cities of residence', 'person schools attended', 'person country of death', 'person children', 'person charges']\n",
      "['organization subsidiaries', 'organization parents', 'organization alternate names', 'organization city of headquarters', 'person siblings']\n",
      "['person country of birth', 'organization website', 'organization shareholders', 'organization dissolved', 'organization founded by']\n",
      "['person cause of death', 'organization political religious affiliation', 'organization stateorprovince of headquarters', 'person other family', 'person city of death']\n",
      "['organization founded', 'person age', 'person city of birth', 'organization members', 'person religion']\n"
     ]
    }
   ],
   "source": [
    "data  = []\n",
    "for steps, (training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations) in enumerate(sampler):\n",
    "            print(current_relations)\n",
    "            data.append((training_data, valid_data, test_data, current_relations, historic_test_data, seen_relations))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from model.bert_encoder import Bert_EncoderMLM\n",
    "model = Bert_EncoderMLM(config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['this is some text' , 'this is some another text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(a, return_tensors='pt', padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No mask token found in the input sequence\n",
      "No mask token found in the input sequence\n"
     ]
    }
   ],
   "source": [
    "outputs = model(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 768])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 2268, 1011, 5757, 1011, 6021, 2102, 2692, 2549, 1024, 5354, 1024, 2184, 4012, 1028, 2626, 1024, 9587, 14945, 25353, 10936, 3669, 25886, 1026, 25353, 10936, 1030, 20643, 9006, 1028, 8299, 1024, 1013, 1013, 7479, 29337, 28251, 8586, 5358, 1013, 3422, 1029, 1058, 1027, 1053, 2063, 2620, 4328, 2629, 2497, 2860, 2683, 16409, 9587, 14945, 25353, 10936, 3669, 25886, 1026, 25353, 10936, 1030, 20643, 9006, 1028, 8299, 1024, 1013, 1013, 2739, 15396, 5643, 9006, 1013, 2739, 1013, 4021, 5643, 1003, 1016, 24700, 7974, 2015, 1013, 6027, 1013, 2466, 1013, 17350, 23809, 2100, 28332, 4274, 2003, 6179, 2005, 2437, 1996, 3606, 2124, 1010, 2758, 2852, 24404, 15222, 2099, 1036, 1036, 15490, 17761, 1010, 2238, 1015, 1011, 1048, 15185, 1011, 30522, 16595, 8067, 30523, 1011, 25269, 2497, 1011, 1011, 102]\n",
      "5\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for j in range(len(data)):\n",
    "    train_data = []\n",
    "    for i in range(len(list(data[j][0].values()))):\n",
    "        train_data.extend(list(data[j][0].values())[i])\n",
    "    cnt = 0\n",
    "    for x in train_data:\n",
    "        if 30524 not in x['tokens']:\n",
    "            cnt +=1\n",
    "            print(x['tokens'])\n",
    "    if cnt > 0:\n",
    "        print(j)\n",
    "        print(cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (942630328.py, line 4)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[71], line 4\u001b[1;36m\u001b[0m\n\u001b[1;33m    delete x\u001b[0m\n\u001b[1;37m           ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.argwhere(tokens == 30524).size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = np.array(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', '2009', '-', '06', '-', '03', '##t', '##0', '##4', ':', '59', ':', '10', 'com', '>', 'wrote', ':', 'mo', '##hd', 'sy', '##az', '##li', 'mahmud', '<', 'sy', '##az', '@', 'yahoo', '##com', '>', 'http', ':', '/', '/', 'www', '##you', '##tub', '##ec', '##om', '/', 'watch', '?', 'v', '=', 'q', '##e', '##8', '##mi', '##5', '##b', '##w', '##9', '##dc', 'mo', '##hd', 'sy', '##az', '##li', 'mahmud', '<', 'sy', '##az', '@', 'yahoo', '##com', '>', 'http', ':', '/', '/', 'news', '##asia', '##one', '##com', '/', 'news', '/', 'asia', '##one', '%', '2', '##bn', '##ew', '##s', '/', 'malaysia', '/', 'story', '/', 'a1', '##stor', '##y', '##200', 'internet', 'is', 'useful', 'for', 'making', 'the', 'truth', 'known', ',', 'says', 'dr', 'maha', '##thi', '##r', '`', '`', 'kuala', 'lumpur', ',', 'june', '1', '-', 'l', '##rb', '-', '[E11]', 'bern', '##ama', '[E12]', '-', 'rr', '##b', '-', '-', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path, additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"])\n",
    "tokens = [101, 2268, 1011, 5757, 1011, 6021, 2102, 2692, 2549, 1024, 5354, 1024, 2184, 4012, 1028, 2626, 1024, 9587, 14945, 25353, 10936, 3669, 25886, 1026, 25353, 10936, 1030, 20643, 9006, 1028, 8299, 1024, 1013, 1013, 7479, 29337, 28251, 8586, 5358, 1013, 3422, 1029, 1058, 1027, 1053, 2063, 2620, 4328, 2629, 2497, 2860, 2683, 16409, 9587, 14945, 25353, 10936, 3669, 25886, 1026, 25353, 10936, 1030, 20643, 9006, 1028, 8299, 1024, 1013, 1013, 2739, 15396, 5643, 9006, 1013, 2739, 1013, 4021, 5643, 1003, 1016, 24700, 7974, 2015, 1013, 6027, 1013, 2466, 1013, 17350, 23809, 2100, 28332, 4274, 2003, 6179, 2005, 2437, 1996, 3606, 2124, 1010, 2758, 2852, 24404, 15222, 2099, 1036, 1036, 15490, 17761, 1010, 2238, 1015, 1011, 1048, 15185, 1011, 30522, 16595, 8067, 30523, 1011, 25269, 2497, 1011, 1011, 102]\n",
    "print(tokenizer.convert_ids_to_tokens(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] 2009 - 06 - 03t04 : 59 : 10 com > wrote : mohd syazli mahmud < syaz @ yahoocom > http : / / wwwyoutubecom / watch? v = qe8mi5bw9dc mohd syazli mahmud < syaz @ yahoocom > http : / / newsasiaonecom / news / asiaone % 2bnews / malaysia / story / a1story200 internet is useful for making the truth known, says dr mahathir ` ` kuala lumpur, june 1 - lrb - [E11] bernama [E12] - rrb - - [SEP]'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = 0\n",
    "for x in train_data:\n",
    "    if 30524 not in x['tokens']:\n",
    "       cnt += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "103"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(config.bert_path, additional_special_tokens=[\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"])\n",
    "tokenizer.convert_tokens_to_ids(\"[MASK]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "text = [\"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [MASK] created the Muppets . [SEP]\", \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [MASK] created the Muppets . [SEP]\"]\n",
    "\n",
    "input_ids = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)\n",
    "output = model(input_ids, return_dict=True)\n",
    "logits = output.logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[101, 100, 100, 102]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 0 is out of bounds for axis 0 with size 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m tokens \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[1;32m----> 2\u001b[0m mask_idx \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margwhere\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m103\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: index 0 is out of bounds for axis 0 with size 0"
     ]
    }
   ],
   "source": [
    "tokens = input_ids.tolist()\n",
    "mask_idx = np.argwhere(np.array(tokens) == 103)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_output = []\n",
    "\n",
    "for i in range(input_ids.shape[0]):\n",
    "    instance_output = torch.index_select(logits, 0, torch.tensor(i))\n",
    "    instance_output = torch.index_select(instance_output, 1, torch.tensor(mask_idx))\n",
    "    mask_output.append(instance_output)\n",
    "mask_output = torch.cat(mask_output, dim=0)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 30522])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodel\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbert_encoder\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Bert_EncoderMLM\n\u001b[0;32m      3\u001b[0m config\u001b[38;5;241m.\u001b[39mpattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mentity_marker_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 4\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mBert_EncoderMLM\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\thanh\\OneDrive\\Desktop\\DATN\\SCKD\\model\\bert_encoder.py:86\u001b[0m, in \u001b[0;36mBert_EncoderMLM.__init__\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28msuper\u001b[39m(Bert_EncoderMLM, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# load model\u001b[39;00m\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder \u001b[38;5;241m=\u001b[39m \u001b[43mBertForMaskedLM\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbert_config \u001b[38;5;241m=\u001b[39m BertForMaskedLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(config\u001b[38;5;241m.\u001b[39mbert_path)\n\u001b[0;32m     89\u001b[0m \u001b[38;5;66;03m# the dimension for the final outputs\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\conpl\\lib\\site-packages\\torch\\nn\\modules\\module.py:918\u001b[0m, in \u001b[0;36mModule.cuda\u001b[1;34m(self, device)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\conpl\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\conpl\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\conpl\\lib\\site-packages\\torch\\nn\\modules\\module.py:810\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[0;32m    809\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[1;32m--> 810\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    812\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[0;32m    813\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[0;32m    814\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[0;32m    821\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\conpl\\lib\\site-packages\\torch\\nn\\modules\\module.py:833\u001b[0m, in \u001b[0;36mModule._apply\u001b[1;34m(self, fn, recurse)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[0;32m    830\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[0;32m    831\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m--> 833\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    834\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[0;32m    835\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\conpl\\lib\\site-packages\\torch\\nn\\modules\\module.py:918\u001b[0m, in \u001b[0;36mModule.cuda.<locals>.<lambda>\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    901\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcuda\u001b[39m(\u001b[38;5;28mself\u001b[39m: T, device: Optional[Union[\u001b[38;5;28mint\u001b[39m, device]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m    902\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Moves all model parameters and buffers to the GPU.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m \n\u001b[0;32m    904\u001b[0m \u001b[38;5;124;03m    This also makes associated parameters and buffers different objects. So\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    916\u001b[0m \u001b[38;5;124;03m        Module: self\u001b[39;00m\n\u001b[0;32m    917\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_apply(\u001b[38;5;28;01mlambda\u001b[39;00m t: \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\conpl\\lib\\site-packages\\torch\\cuda\\__init__.py:289\u001b[0m, in \u001b[0;36m_lazy_init\u001b[1;34m()\u001b[0m\n\u001b[0;32m    284\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    285\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    286\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmultiprocessing, you must use the \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m start method\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    287\u001b[0m     )\n\u001b[0;32m    288\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch\u001b[38;5;241m.\u001b[39m_C, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_cuda_getDeviceCount\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 289\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTorch not compiled with CUDA enabled\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    291\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[0;32m    292\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    293\u001b[0m     )\n",
      "\u001b[1;31mAssertionError\u001b[0m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "import model.bert_encoder\n",
    "from model.bert_encoder import Bert_EncoderMLM\n",
    "config.pattern = \"entity_marker_mask\"\n",
    "model = Bert_EncoderMLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import Config\n",
    "config = Config('config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Config' object has no attribute 'infonce_temprature'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfonce_temprature\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Config' object has no attribute 'infonce_temprature'"
     ]
    }
   ],
   "source": [
    "config.infonce_temprature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "additional_special_tokens = [\"[E11]\", \"[E12]\", \"[E21]\", \"[E22]\"]\n",
    "additional_special_tokens.extend([f\"[REL{i}]\" for i in range(1, 50 + 1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', additional_special_tokens=additional_special_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30526"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(\"[REL1]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "m = nn.Softmax(dim=1)\n",
    "input = torch.randn(2, 3) + 1e8\n",
    "output = m(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3333, 0.3333, 0.3333],\n",
       "        [0.3333, 0.3333, 0.3333]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 5])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = torch.rand(1,3000)\n",
    "C = torch.rand(5,768)\n",
    "W = torch.rand(3000,768)\n",
    "output = torch.matmul(V,W)\n",
    "output = torch.matmul(output,C.T)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(1,1)\n",
    "b = torch.rand(1,5)\n",
    "temp = torch.cat([a,b],dim=1).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = nn.Softmax(dim=0)\n",
    "output = m(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1380, 0.1012, 0.2047, 0.1749, 0.1983, 0.1830])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(1.0)\n",
    "b = torch.rand(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000, 0.0674, 0.6470, 0.4542])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([a.unsqueeze(0),b],dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1736, 0.1032, 0.3244, 0.1839, 0.0675, 0.1473])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = torch.tensor([61.01224136352539 ,-23.5942, 162.7764,  70.3629, -92.8090,  34.2552])\n",
    "m(h/abs(h).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.3748, -0.1449,  1.0000,  0.4323, -0.5702,  0.2104])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h/abs(h).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 61.0122,  23.5942, 162.7764,  70.3629,  92.8090,  34.2552])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abs(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer , BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [\"This is a sample text\", \"This is another sample text\"]\n",
    "inputs = tokenizer(a, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(inputs['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 7, 768])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conpl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
